{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e64327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /var/folders/_x/fh3t8wcj3_xbs4fddcxmbpc00000gn/T/matplotlib-sp1lhqhg because the default path (/Users/hansshen/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hansshen/anaconda3/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#set up, main tools= np,pd,tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "\n",
    "# Add some convenience functions to Pandas DataFrame.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "def mask(df, key, function):\n",
    "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
    "  return df[function(df[key])]\n",
    "\n",
    "def flatten_cols(df):\n",
    "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "  return df\n",
    "\n",
    "pd.DataFrame.mask = mask\n",
    "pd.DataFrame.flatten_cols = flatten_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe1d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prelims, just giving a demonstration of the input vector space \n",
    "def split_df(df,testing_fraction=0.2):\n",
    "    test=df.sample(frac=testing_fraction,replace=False)\n",
    "    train=df[~df.index.isin(test.index)]\n",
    "    return train, test\n",
    "\n",
    "mood=list(range(10))\n",
    "energy_levels=list(range(5))\n",
    "talk_preferences=[0,1]\n",
    "internal_journeys=[\"winding_road\",\"steep_staircase\",\"flowing_river\",\"open_field\"]\n",
    "user_cols=[\"mood\",\"energy_level\",\"talk_preference\",\"internal_journey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fa7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data of our users, connected to the backend database \n",
    "user_info=\"link_to_backend\"\n",
    "df_user=pd.DataFrame(user_info,columns=user_cols)\n",
    "\n",
    "#Interactive Altair visualization Tool, helpful when we have a large user database\n",
    "#display histograms of data,sliced by a given attribute\n",
    "\n",
    "#first, create a filter\n",
    "major_filter=alt.selection_multi(fields=[\"Major\"])\n",
    "major_chart=alt.Chart().mark_bar().encode(\n",
    "    x=\"count()\",\n",
    "    y=alt.Y(\"Major:N\"),\n",
    "    color=alt.condition(major_filter,\n",
    "                        alt.Color(\"Major:N\",scale=alt.Scale(scheme='category20')),\n",
    "                        alt.value(\"lightgray\"))\n",
    ").properties(width=300,height=300,selection=major_filter)\n",
    "\n",
    "# A function that generates a histogram of filtered data.\n",
    "def filtered_hist(field, label, filter):\n",
    "  \"\"\"Creates a layered chart of histograms.\n",
    "  The first layer (light gray) contains the histogram of the full data, and the\n",
    "  second contains the histogram of the filtered data.\n",
    "  Args:\n",
    "    field: the field for which to generate the histogram.\n",
    "    label: String label of the histogram.\n",
    "    filter: an alt.Selection object to be used to filter the data.\n",
    "  \"\"\"\n",
    "  base = alt.Chart().mark_bar().encode(\n",
    "      x=alt.X(field, bin=alt.Bin(maxbins=20), title=label),\n",
    "      y=\"count()\").properties(width=300)\n",
    "  return alt.layer(\n",
    "      base.transform_filter(filter),\n",
    "      base.encode(color=alt.value('lightgray'), opacity=alt.value(.7)),\n",
    "  ).resolve_scale(y='independent')\n",
    "\n",
    "# Create a chart for the count, and one for the mean.\n",
    "alt.hconcat(\n",
    "    filtered_hist('num_mentor', 'num_mentor', major_filter),\n",
    "    filtered_hist('avg_rating', 'avg_rating', major_filter),\n",
    "    major_chart,\n",
    "    data=df_mentee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af601f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual Machine Learning\n",
    "def sparse_tensor(ratings_df):\n",
    "    indices=ratings_df[[\"mentor_id\",\"mentee_id\"]].values\n",
    "    values=ratings_df[\"score\"].tolist()\n",
    "    dense_shape=[num_mentor,num_mentee+1]\n",
    "    return tf.SparseTensor(indices=indices,\n",
    "                           values=values,\n",
    "                           dense_shape=dense_shape)\n",
    "def sparse_mse_loss(sparse_ratings,mentor_embeddings,mentee_embeddings):\n",
    "    predictions = tf.gather_nd(\n",
    "      tf.matmul(mentor_embeddings, mentee_embeddings, transpose_b=True),\n",
    "      sparse_ratings.indices)\n",
    "    loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "    return loss\n",
    "#collaborative filtering(CF)\n",
    "class CF_model(object):\n",
    "    def __init__(self,embedding_vars,loss,metrics=None):\n",
    "        #embedding_Vars= dict of tf.Variable\n",
    "        #loss: a float tensor to optimize\n",
    "        #metric optional list of dict of tensors, which would \n",
    "        #be separately plotted during training\n",
    "        self._embedding_vars=embedding_vars\n",
    "        self._loss=loss\n",
    "        self._metrics=metrics\n",
    "        self._embeddings={k: None for k in embedding_vars}\n",
    "        self._session=None\n",
    "    #why do we need property decorator here?\n",
    "    #returns the metrics dictionary valauted at the last iteration\n",
    "    @property \n",
    "    def embeddings(self):\n",
    "        return self._embeddings\n",
    "    def train(self,num_iterations=100,\n",
    "              learning_rate=1,\n",
    "              plot_results=True,\n",
    "              optimizer=tf.train.GradientDescentOptimizer):\n",
    "        with self._loss.graph.as_default():\n",
    "            opt=optimizer(learning_rate)\n",
    "            train_op=opt.minimize(self._loss)\n",
    "            local_init_op=tf.group(\n",
    "                tf.variables_initializer(opt.variables()),\n",
    "                tf.local_variables_initializer()\n",
    "            )\n",
    "            if self._session is None:\n",
    "                self._session=tf.Session()\n",
    "                with self._session.as_default():\n",
    "                    self._session.run(tf.global_variables_initializer())\n",
    "                    self._session.run(tf.tables_initializer())\n",
    "                    tf.train.start_queue_runners()\n",
    "        with self._session.as_default():\n",
    "            local_init_op.run()\n",
    "            iterations=[]\n",
    "            metrics=self._metrics or ({},)\n",
    "            metrics_vals=[collections.defaultdict(list) for _ in self._metrics]\n",
    "    # Training and appending results\n",
    "            for i in range(num_iterations+1):\n",
    "                _,results=self._session.run((train_op,metrics))\n",
    "                if (i % 10 == 0) or i == num_iterations:\n",
    "                    print(\"\\r iteration %d: \" % i + \", \".join(\n",
    "                            [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
    "                            end='')\n",
    "                    iterations.append(i)\n",
    "                    for metric_val, result in zip(metrics_vals, results):\n",
    "                        for k, v in result.items():\n",
    "                            metric_val[k].append(v)\n",
    "            for k, v in self._embedding_vars.items():\n",
    "                self._embeddings[k] = v.eval()\n",
    "\n",
    "            #plot the metrics\n",
    "            if plot_results:\n",
    "                num_subplots=len(metrics)+1\n",
    "                fig=plt.figure()\n",
    "                fig.set_size_inches(num_subplots*10,8)\n",
    "                for i,metric_vals in enumerate(metrics_vals):\n",
    "                    ax=fig.add_subplot(1,num_subplots,i+1)\n",
    "                    for k,v in metric_vals.items():\n",
    "                        ax.plot(iterations,v,label=k)\n",
    "                    ax.set_xlim([1,num_iterations])\n",
    "                    ax.legend()\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization\n",
    "#input: two embedding matrices, output: loss\n",
    "def gravity(U,V):\n",
    "    return 1. / (U.shape[0].value*V.shape[0].value) * tf.reduce_sum(\n",
    "      tf.matmul(U, U, transpose_a=True) * tf.matmul(V, V, transpose_a=True))\n",
    "def regular(U,V):\n",
    "    loss_U=1/(U.shape[0].value)*tf.reduce_sum(U*U)\n",
    "    loss_V=1/(V.shape[0].value)*tf.reduce_sum(V*V)\n",
    "    return loss_U+loss_V\n",
    "def build_regularized_model(ratings,embedding_dim=3,reg_coef=1,grav_coef=1,init_stddev=0.5):\n",
    "    train,test=split_df(ratings)\n",
    "    A_train=sparse_tensor(train)\n",
    "    A_test=sparse_tensor(test)\n",
    "    U=tf.Variable(tf.random_normal(\n",
    "                            [A_train.dense_shape[0],embedding_dim],stddev=init_stddev)\n",
    "                 )\n",
    "    V=tf.Variable(tf.random_normal(\n",
    "                            [A_train.dense_shape[1],embedding_dim],stddev=init_stddev)\n",
    "                 )\n",
    "    error_train=sparse_mse_loss(A_train,U,V)\n",
    "    error_test=sparse_mse_loss(A_test,U,V)\n",
    "    reg_loss=reg_coef*regular(U,V)\n",
    "    grav_loss=grav_coef*gravity(U,V)\n",
    "    total_loss=error_train+reg_loss+grav_loss\n",
    "    losses={\n",
    "        'train_error':error_train,\n",
    "        'test_error':error_test\n",
    "    }\n",
    "    loss_components ={\n",
    "        \"observed_loss\":error_train,\n",
    "        \"regularization_loss\":reg_loss,\n",
    "        \"gravity_loss\":grav_loss\n",
    "    }\n",
    "    embeddings={\"mentor_id\":U,\"mentee_id\":V}\n",
    "    return CF_model(embeddings,total_loss,[losses,loss_components])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
